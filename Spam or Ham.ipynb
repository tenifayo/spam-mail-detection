{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam Detection using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mail type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "message",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "00a61a65-30e7-4425-bc9d-97d52fecbbcf",
       "rows": [
        [
         "0",
         "ham",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."
        ],
        [
         "1",
         "ham",
         "Ok lar... Joking wif u oni..."
        ],
        [
         "2",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"
        ],
        [
         "3",
         "ham",
         "U dun say so early hor... U c already then say..."
        ],
        [
         "4",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though"
        ],
        [
         "5",
         "spam",
         "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv"
        ],
        [
         "6",
         "ham",
         "Even my brother is not like to speak with me. They treat me like aids patent."
        ],
        [
         "7",
         "ham",
         "As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune"
        ],
        [
         "8",
         "spam",
         "WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only."
        ],
        [
         "9",
         "spam",
         "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030"
        ],
        [
         "10",
         "ham",
         "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today."
        ],
        [
         "11",
         "spam",
         "SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info"
        ],
        [
         "12",
         "spam",
         "URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18"
        ],
        [
         "13",
         "ham",
         "I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times."
        ],
        [
         "14",
         "ham",
         "I HAVE A DATE ON SUNDAY WITH WILL!!"
        ],
        [
         "15",
         "spam",
         "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL"
        ],
        [
         "16",
         "ham",
         "Oh k...i'm watching here:)"
        ],
        [
         "17",
         "ham",
         "Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet."
        ],
        [
         "18",
         "ham",
         "Fine if thats the way u feel. Thats the way its gota b"
        ],
        [
         "19",
         "spam",
         "England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+"
        ],
        [
         "20",
         "ham",
         "Is that seriously how you spell his name?"
        ],
        [
         "21",
         "ham",
         "Im going to try for 2 months ha ha only joking"
        ],
        [
         "22",
         "ham",
         "So ü pay first lar... Then when is da stock comin..."
        ],
        [
         "23",
         "ham",
         "Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?"
        ],
        [
         "24",
         "ham",
         "Ffffffffff. Alright no way I can meet up with you sooner?"
        ],
        [
         "25",
         "ham",
         "Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worried. He knows I'm sick when I turn down pizza. Lol"
        ],
        [
         "26",
         "ham",
         "Lol your always so convincing."
        ],
        [
         "27",
         "ham",
         "Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?"
        ],
        [
         "28",
         "ham",
         "I'm back &amp; we're packing the car now, I'll let you know if there's room"
        ],
        [
         "29",
         "ham",
         "Ahhh. Work. I vaguely remember that! What does it feel like? Lol"
        ],
        [
         "30",
         "ham",
         "Wait that's still not all that clear, were you not sure about me being sarcastic or that that's why x doesn't want to live with us"
        ],
        [
         "31",
         "ham",
         "Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won't go there! Not doing too badly cheers. You? "
        ],
        [
         "32",
         "ham",
         "K tell me anything about you."
        ],
        [
         "33",
         "ham",
         "For fear of fainting with the of all that housework you just did? Quick have a cuppa"
        ],
        [
         "34",
         "spam",
         "Thanks for your subscription to Ringtone UK your mobile will be charged £5/month Please confirm by replying YES or NO. If you reply NO you will not be charged"
        ],
        [
         "35",
         "ham",
         "Yup... Ok i go home look at the timings then i msg ü again... Xuhui going to learn on 2nd may too but her lesson is at 8am"
        ],
        [
         "36",
         "ham",
         "Oops, I'll let you know when my roommate's done"
        ],
        [
         "37",
         "ham",
         "I see the letter B on my car"
        ],
        [
         "38",
         "ham",
         "Anything lor... U decide..."
        ],
        [
         "39",
         "ham",
         "Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!"
        ],
        [
         "40",
         "ham",
         "Pls go ahead with watts. I just wanted to be sure. Do have a great weekend. Abiola"
        ],
        [
         "41",
         "ham",
         "Did I forget to tell you ? I want you , I need you, I crave you ... But most of all ... I love you my sweet Arabian steed ... Mmmmmm ... Yummy"
        ],
        [
         "42",
         "spam",
         "07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow"
        ],
        [
         "43",
         "ham",
         "WHO ARE YOU SEEING?"
        ],
        [
         "44",
         "ham",
         "Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches..."
        ],
        [
         "45",
         "ham",
         "No calls..messages..missed calls"
        ],
        [
         "46",
         "ham",
         "Didn't you get hep b immunisation in nigeria."
        ],
        [
         "47",
         "ham",
         "Fair enough, anything going on?"
        ],
        [
         "48",
         "ham",
         "Yeah hopefully, if tyler can't do it I could maybe ask around a bit"
        ],
        [
         "49",
         "ham",
         "U don't know how stubborn I am. I didn't even want to go to the hospital. I kept telling Mark I'm not a weak sucker. Hospitals are for weak suckers."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5572
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mail type</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mail type                                            message\n",
       "0          ham  Go until jurong point, crazy.. Available only ...\n",
       "1          ham                      Ok lar... Joking wif u oni...\n",
       "2         spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3          ham  U dun say so early hor... U c already then say...\n",
       "4          ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...        ...                                                ...\n",
       "5567      spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568       ham               Will ü b going to esplanade fr home?\n",
       "5569       ham  Pity, * was in mood for that. So...any other s...\n",
       "5570       ham  The guy did some bitching but I acted like i'd...\n",
       "5571       ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam_or_ham.csv',encoding= 'latin1')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: mail type, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mail type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the labels\n",
    "le = LabelEncoder()\n",
    "df['mail type'] = le.fit_transform(df['mail type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]+', ' ',text) #to remove punctuation\n",
    "    text = re.sub(r'\\d+(\\.\\d+)?', ' ', text)#to remove numbers\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])#to remove stopwords\n",
    "    text = ' '.join([lemmatizer.lemmatize(y,'v') for y in text.split()]) #to lemmatize\n",
    "    text = ' '.join([lemmatizer.lemmatize(y,'n') for y in text.split()]) #to lemmatize\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_message'] = df['message'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "mail type",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "message",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cleaned_message",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "cfb85289-c258-4dde-a058-0d1be53a354c",
       "rows": [
        [
         "0",
         "0",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...",
         "go jurong point crazy available bugis n great world la e buffet cine get amore wat"
        ],
        [
         "1",
         "0",
         "Ok lar... Joking wif u oni...",
         "ok lar joke wif u oni"
        ],
        [
         "2",
         "1",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's",
         "free entry wkly comp win fa cup final tkts st may text fa receive entry question std txt rate c apply"
        ],
        [
         "3",
         "0",
         "U dun say so early hor... U c already then say...",
         "u dun say early hor u c already say"
        ],
        [
         "4",
         "0",
         "Nah I don't think he goes to usf, he lives around here though",
         "nah think go usf live around though"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mail type</th>\n",
       "      <th>message</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah think go usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mail type                                            message  \\\n",
       "0          0  Go until jurong point, crazy.. Available only ...   \n",
       "1          0                      Ok lar... Joking wif u oni...   \n",
       "2          1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3          0  U dun say so early hor... U c already then say...   \n",
       "4          0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                     cleaned_message  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                              ok lar joke wif u oni  \n",
       "2  free entry wkly comp win fa cup final tkts st ...  \n",
       "3                u dun say early hor u c already say  \n",
       "4                nah think go usf live around though  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df[\"cleaned_message\"], df[\"mail type\"], \n",
    "                                                                      test_size=0.2, random_state=42,\n",
    "                                                                      stratify=df['mail type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "word_counts = Counter([word for text in train_texts for word in text.split(' ') ])\n",
    "vocab = {word: idx + 1 for idx, (word, _) in enumerate(word_counts.most_common())}  # Start indexing from 1\n",
    "\n",
    "# Convert text to sequence of indices\n",
    "def text_to_sequence(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    seq = [vocab.get(word, 0) for word in tokens]\n",
    "    return  seq \n",
    "\n",
    "train_sequences = [text_to_sequence(text) for text in train_texts]\n",
    "test_sequences = [text_to_sequence(text) for text in test_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 1115)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sequences), len(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 80  # Truncate or pad sequences to this length\n",
    "\n",
    "def pad_sequences(sequences: list, max_len=MAX_LEN):\n",
    "    \"\"\"This function takes a sequence and either truncates it or pads it with zeros \n",
    "    to make it the same length as the max_len\"\"\"\n",
    "    return [seq[:max_len] + [0] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in sequences]\n",
    "\n",
    "train_sequences = pad_sequences(train_sequences)\n",
    "test_sequences = pad_sequences(test_sequences)\n",
    "\n",
    "# Convert the sequence to tensors\n",
    "train_tensor = torch.tensor(train_sequences, dtype=torch.long)\n",
    "test_tensor = torch.tensor(test_sequences, dtype=torch.long)\n",
    "train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.float32)\n",
    "test_labels_tensor = torch.tensor(test_labels.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Custom pytorch Dataset\n",
    "class MessageTextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MessageTextDataset(train_tensor, train_labels_tensor)\n",
    "test_dataset = MessageTextDataset(test_tensor, test_labels_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam classifier model using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNNModel(\n",
      "  (embedding): Embedding(5706, 32)\n",
      "  (rnn1): RNN(32, 32, batch_first=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (rnn2): RNN(32, 64, batch_first=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=5120, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Spam classifier model using RNN\n",
    "\n",
    "class SimpleRNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size =len(vocab) + 1, embedding_dim=32, hidden_dim1=32, hidden_dim2=64, sequence_length=80):\n",
    "        super(SimpleRNNModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.rnn1 = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2) \n",
    "\n",
    "        self.rnn2 = nn.RNN(input_size=hidden_dim1, hidden_size=hidden_dim2, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim2 * sequence_length, 1)  \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x, _ = self.rnn1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x, _ = self.rnn2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "vocab_size = len(vocab) + 1 \n",
    "sequence_length = 80\n",
    "rnn_model = SimpleRNNModel(vocab_size=vocab_size, sequence_length=sequence_length)\n",
    "\n",
    "print(rnn_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2313\n",
      "Epoch 2, Loss: 0.1087\n",
      "Epoch 3, Loss: 0.0677\n",
      "Epoch 4, Loss: 0.0470\n",
      "Epoch 5, Loss: 0.0375\n",
      "Epoch 6, Loss: 0.0203\n",
      "Epoch 7, Loss: 0.0307\n",
      "Epoch 8, Loss: 0.0158\n",
      "Epoch 9, Loss: 0.0103\n",
      "Epoch 10, Loss: 0.0073\n",
      "Epoch 11, Loss: 0.0056\n",
      "Epoch 12, Loss: 0.0032\n",
      "Epoch 13, Loss: 0.0084\n",
      "Epoch 14, Loss: 0.0081\n",
      "Epoch 15, Loss: 0.0071\n",
      "Epoch 16, Loss: 0.0013\n",
      "Epoch 17, Loss: 0.0008\n",
      "Epoch 18, Loss: 0.0028\n",
      "Epoch 19, Loss: 0.0032\n",
      "Epoch 20, Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rnn_model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    rnn_model.train()\n",
    "    epoch_loss = 0\n",
    "    for texts, labels in train_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = rnn_model(texts)\n",
    "        labels = labels.view(-1, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9821\n",
      "Precision: 0.9708\n",
      "Recall: 0.8926\n",
      "F1 Score: 0.9301\n"
     ]
    }
   ],
   "source": [
    "rnn_model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        preds = rnn_model(texts).squeeze().round() \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99       966\n",
      "         1.0       0.97      0.89      0.93       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.94      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: torch.nn.Module,\n",
    "               target_dir: str,\n",
    "               model_name: str):\n",
    "\n",
    "  # target directory\n",
    "  target_dir_path = Path(target_dir)\n",
    "  target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok=True)\n",
    "\n",
    "  # createv model save path\n",
    "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "  model_save_path = target_dir_path / model_name\n",
    "\n",
    "  # Save the model state_dict()\n",
    "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "  torch.save(obj=model.state_dict(),\n",
    "             f=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving model to: models\\spam_classifier_rnn.pt\n"
     ]
    }
   ],
   "source": [
    "save_model(model=rnn_model,\n",
    "           target_dir='models',\n",
    "           model_name='spam_classifier_rnn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'models/spam_classifier_rnn.pt'\n",
    "\n",
    "model = SimpleRNNModel()\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(message: str):\n",
    "    message = preprocess_text(message)\n",
    "    seq = text_to_sequence(message)\n",
    "    padded_seq = pad_sequences([seq])\n",
    "    seq_tensor = torch.tensor(padded_seq, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        text = seq_tensor.squeeze(1).to(device)\n",
    "        prediction = model(text).squeeze().round()\n",
    "        mail_type = le.inverse_transform([int(prediction.item())]).item()\n",
    "        # mail_type =  \"Spam\" if prediction == 1 else \"Ham\"\n",
    "    \n",
    "    return mail_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"You have an important customer service announcement from PREMIER. Call FREEPHONE 0800 542 0578 now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"I am going to the office now. I will call you later.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
